{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Intro","text":"<p>Welcome to the Mykomap docs!</p>"},{"location":"api/","title":"API","text":"<p>The Mykomap API is used by front-end clients to interact with the back-end server.</p>"},{"location":"api/#ts-rest","title":"ts-rest","text":"<p>We define and implement the API using a lightweight library called ts-rest.</p> <p>The API is defined by the <code>contract.ts</code> file in the <code>@mykomap/common</code> library. In the contract, we list the API routes, including details about the requests (e.g. query parameters) and responses and their types (using Zod schemas).</p> <p>In the front-end and back-end, we use the <code>ts-rest</code> client and server libraries respectively. They consume the <code>contract.ts</code> and provide fully type-safe API methods in Typescript, with runtime validation based on the Zod schemas.</p> <p>From the <code>contract.ts</code>, we can also generate an OpenAPI spec, using the <code>generate-openapi</code> script.</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#key-requirements","title":"Key requirements","text":"<p>Before diving in to the architecture, let's outline some key requirements &amp; design decisions that led to this architecture.</p> <ul> <li>We need to display hundreds of thousands of markers, and for these to load as quickly as possible with a slow connection</li> <li>Searching and filtering the results must be fast</li> </ul> <p>For these reasons, serving all the data straight to a front-end is not viable. In order to reduce loading times and computational load on the clients, we need a back-end that has access to all the data and can perform searches on it, then serves the front-end clients with the minimum required data it needs to render the UI.</p> <p>NOTE: In order to minimise the amount of data served to the front-end, we don't include dataset item IDs, only an array of their locations. Therefore the front-end generally refers to items by their array index. An item's index is persistent in the short term, e.g. within a single browser session, but can change when new data is loaded into the back-end. An item's ID is more stable, ideally permanent.</p>"},{"location":"architecture/#system-architecture","title":"System architecture","text":""},{"location":"architecture/#front-end-architecture","title":"Front-end architecture","text":""},{"location":"architecture/#notes","title":"Notes","text":"<ul> <li> <p>We adhere to the Redux principle of having a single source of truth. Breaking this principle is the biggest source of poor spaghetti code and bugs. In practice, it means that before tracking any new state (i.e. any information/data served by the back-end or arising from a user action), pause and think:</p> </li> <li> <p>Is this actually new state, or can it be derived from state that we are already storing somewhere?</p> </li> <li> <p>If it is actually new state, where is the best place for it to live? This should be a single place. Usually it is the Redux store or, if it is contained within a single UI component and the rest of the app doesn't need to know about it, it can be internal React state within that component.</p> </li> <li> <p>We create a plain MapLibreGL component and use its API directly, rather than using a binding such as <code>react-map-gl</code>. Although this   would integrate more nicely with React and Redux hooks, it adds overhead and we can't guarantee that the binding library will always be   maintained. Instead, we simply pass marker data and MapLibre click events through a MapWrapper React component, as follows:</p> </li> <li>MapWrapper holds a reference to a MapLibre object. It can interact with this object to directly set the     GeoJSON data that it renders. It sends custom events, including data, to the MapLibre object by calling <code>fire(\"eventName\")</code> on the object.</li> <li>The <code>mapLibre.ts</code> file holds the code internal to the MapLibre object. It can react to events fired by MapWrapper with the <code>map.on(\"eventName\", callback)</code> listener. And it can fire events back to MapWrapper by calling the callbacks that were passed into the <code>createMap</code> function.</li> </ul>"},{"location":"architecture/#back-end-architecture","title":"Back-end architecture","text":""},{"location":"architecture/#dataset-files","title":"Dataset files","text":"<p>All persistent data is stored on the back-end server as JSON files, in the following folder structure as seen from the SERVER_DATA_ROOT location:</p> <pre><code>\u251c\u2500\u2500 datasets\n\u2502   \u251c\u2500\u2500 some-dataset\n\u2502   \u2502   \u251c\u2500\u2500 config.json (itemProps, vocabs, UI config, languages, etc.)\n\u2502   \u2502   \u251c\u2500\u2500 about.md (markdown file containing info to be displayed in AboutPanel)\n\u2502   \u2502   \u251c\u2500\u2500 locations.json (array of lng-lat coordinates for each item)\n\u2502   \u2502   \u251c\u2500\u2500 searchable.json (array of the property values and searchable strings for each item)\n\u2502   \u2502   \u251c\u2500\u2500 items\n\u2502   \u2502   |   \u251c\u2500\u2500 0.json (full info of first item in the above aggregate JSONs)\n\u2502   \u2502   |   \u251c\u2500\u2500 1.json\n\u2502   \u2502   |   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 other-dataset\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 ...\n</code></pre> <p>Note that the <code>config.json</code> for each dataset is kept in source control in the <code>@mykomap/config</code> library (to be implemented).</p> <p>See the <code>back-end test data</code> for example file contents.</p>"},{"location":"architecture/#potential-optimisation","title":"Potential optimisation:","text":"<p>The <code>searchable.json</code> will be loaded into the back-end server's memory. Since there will be one row per item, with 100k items, every 10 characters adds a new megabyte. The really bulky bit is the text searchString part, so maybe it could be kept in its own plain text file, with one line per item. Searching it could be done by streaming it from disk, which avoids loading the entire file permanently into memory (for each dataset).</p> <p>For instance, this SO thread has some sample stream-searching code, and a reference to a module which performs the streaming by what appears to be a fast non-buffering algorithm.</p>"},{"location":"architecture/#data-generation","title":"Data generation","text":"<p>These directories of JSONs, including the searchable strings in the <code>searchable.json</code> files, need to be pre-generated by a script. This script will be written in JS/TS and live in the monorepo, to be run on the back-end server.</p> <p>The script will take the full data CSV for a map (generated by the data factory) as inputs, and write the full data into the required JSON files in the directory structure specified above.</p>"},{"location":"architecture/#note","title":"Note:","text":"<p>We will need to manually copy the <code>standard.csv</code> from the data factory server to the back-end. Maybe in the future, the data factory pipeline can be enhanced to write the JSON files to the back-end server so that no manual duplication is necessary (and maybe we can eventually get rid of the separate data server altogether). Or, the bacl-end server could be given a URL to the appropriate <code>standard.csv</code> file(s) as published by the data factory and download it from there as part of a <code>build-data</code> script (possibly when notified by a webhook, or possibly polling and checking the file modification date)</p>"},{"location":"architecture/#dataset-instances","title":"Dataset instances","text":"<ul> <li> <p>For each dataset available in the <code>datasets</code> directory on server start, a dataset instance is created   by the Dataset service. Each Dataset instance has a:</p> </li> <li> <p><code>getItem</code> method</p> </li> <li><code>getConfig</code> method, which includes the vocabs</li> <li><code>getAbout</code> method</li> <li><code>getLocations</code> method, which returns a stream of the data</li> <li><code>search</code> method, which iterates through the data loaded from <code>searchable.json</code> to find matching items</li> </ul>"},{"location":"automated-testing/","title":"Automated Testing","text":""},{"location":"automated-testing/#unit-tests","title":"Unit Tests","text":"<ul> <li>Run <code>npm test</code> to run all UTs in the monorepo. The UTs also run in our continuous integration (CI) pipeline, using GitHub Actions.</li> <li>We use Vitest for all our tests, which provides a way to use mocks, spies, assertions, etc. We also have React Testing Library   set up on the front-end for our React components, but we don't have many tests for these. In the front-end, UT focus is on   our Redux slices, which are where most logic happens.</li> </ul>"},{"location":"automated-testing/#what-is-a-unit-test","title":"What is a unit test?","text":"<p>A unit test instantiates a unit (a small portion, usually a function) of our app and verifies its behaviour independently from other parts. It is narrow in scope and ideally covers all cases, ensuring that the unit behaves correctly. There are 3 stages (3 As) in a well-structured unit test:</p> <ol> <li> <p>Arrange: Set up the unit to be tested and the environment.</p> </li> <li> <p>Act: Invoke the actual function under test.</p> </li> <li> <p>Assert: Make a claim (usually just one) about the unit and/or verify its interaction with a test object.</p> </li> </ol>"},{"location":"automated-testing/#tips-for-writing-a-good-test","title":"Tips for writing a good test","text":"<p>A good unit test is:</p> <ul> <li> <p>Readable. It should be clear which scenario is being tested and\u2014 if the test fails\u2014 easy to detect how to address the problem. It should be well-structured and include comments where needed. This will make the test easier to maintain.</p> </li> <li> <p>Reliable. It should fail only if there\u2019s a bug in the code they're testing. A bad test may pass when running one-by-one, but fail when running the whole test suite, or pass on our development machine and fail on the continuous integration server. A good test is repeatable in any environment or running order.</p> </li> <li> <p>Fast. It should run in a few milliseconds. Avoid writing tests with timeouts and instead look for ways to fake the system clock if needed. All the tests should be run often, on our own machines and in the CI pipeline, to check that no bugs have been introduced. Slow tests will discourage this.</p> </li> <li> <p>Isolated. Its result should only depend on the unit being tested. To eliminate the influence of external factors, both the test and the unit under test should not access network resources, databases, file system, etc. A good test replaces external interfaces with test objects (e.g. fakes, stubs) that we can control.</p> </li> </ul>"},{"location":"automated-testing/#behaviour-driven-development-bdd","title":"Behaviour-driven development (BDD)","text":"<p>When adding a new feature, we write the code and unit tests together using behaviour-driven development (BDD). The technique produces specifications of the code in non-technical language, making it more understandable, and encourages writing testable, modularised code.</p> <p>Here is a useful guide on BDD. The usual development flow, which is explained in more detail in the guide, is as follows:</p> <ol> <li>An initial spec is written, with tests for the most basic functionality.</li> <li>An initial implementation is created.</li> <li>We run the tests to check whether the code works. While the functionality is not complete, errors are displayed. We make corrections until the tests pass.</li> <li>Now we have a working initial implementation with tests.</li> <li>We add more use cases to the spec, probably not yet supported by the implementations. Tests start to fail.</li> <li>Go to 3, update the implementation till tests give no errors.</li> <li>Repeat steps 3-6 till the functionality is ready.</li> </ol>"},{"location":"automated-testing/#writing-a-ut-in-the-mm-repo","title":"Writing a UT in the MM repo","text":"<ul> <li>Add UTs to a <code>.test.ts</code> file in the same directory as the code you are testing.</li> <li>In VS Code, install the extensions:</li> <li>Vitest Snippets to make writing test boilerplate code faster</li> <li>Vitest to run individual tests from sidebar. Note, this extension sometimes doesn't catch exceptions that occur between tests, so you should also occasionally run <code>npm test</code> manually.</li> </ul>"},{"location":"config/","title":"Configuration Reference","text":"<p>Here is a reference for all the options in a map's <code>config.json</code> file, and what they are used for.</p> <p>You can also find a working example in the <code>back-end test data</code>.</p> <p>TODO: Fill this out for all config options. We currently just have a reference for popup config (W.I.P.)</p>"},{"location":"config/#popup-layout","title":"Popup layout","text":"<p>The <code>popup</code> config field is used to define the layout of the popups, which are displayed when items are clicked on the map. The layout config is declarative and designed to be as simple as possible, with the front-end handling the conversion of the config into MUI React components.</p> <p>There are 4 subfields:</p> <ul> <li><code>titleProp</code> (required): The ID of the itemProp that should be used as the popup's title</li> <li><code>leftPane</code>: a list of itemProps to be displayed in the popup's left pane, from top-to-bottom in the layout, including their style definition (see below for more details).</li> <li><code>topRightPane</code>: same as <code>leftPane</code> but for the top-right pane of the popup</li> <li><code>bottomRightPane</code>: same as <code>leftPane</code> but for the bottom-right pane of the popup</li> </ul> <p>And these are the different options to define the style of an itemProp:</p> <ul> <li><code>valueStyle</code> (defaults to <code>text</code>): This can be one of the following options:</li> <li><code>text</code>: The value(s) of the itemProp are displayed as plain text.</li> <li><code>address</code>: Used in the case of the itemProp value being an address with comma separators (it must be formatted like this upstream). Each address section is displayed on a new line.</li> <li><code>hyperlink</code>: The value(s) of the itemProp are displayed as clickable hyperlinks.</li> <li><code>showBullets</code> (defaults to <code>false</code>): If an itemProp has multiple values, they are always displayed on multiple lines. If this is set option to true, bullet points will be shown before each value.</li> <li><code>singleColumnLimit</code> (number): If specified, a list will split into 2 columns if the number of values is more than this.</li> <li><code>showLabel</code> (defaults to <code>false</code>): If this is set to true, the label of the itemProp (taken from <code>titleUri</code> in the itemProp definition) is displayed as a heading above the value(s).</li> <li><code>hyperlinkBaseUri</code> (defaults to None): If <code>valueStyle</code> is <code>hyperlink</code>, this base URI is prepended to the itemProp value, to form the href.</li> <li><code>displayText</code> (defaults to None): If <code>valueStyle</code> is <code>hyperlink</code>, this sets the display text of the hyperlink.</li> <li><code>multiple</code> (defaults to false): If this is true, the value is an array.</li> </ul> <pre><code>\"popup\": {\n    \"titleProp\": \"name\",\n    \"leftPane\": [\n      {\"itemProp\": \"category\", \"valueStyle\": \"text\", \"showLabel\": true },\n      {\"itemProp\": \"description\", \"valueStyle\": \"text\" },\n      {\"itemProp\": \"website\", \"valueStyle\": \"hyperlink\", \"displayText\": \"Website\",  }\n    ],\n    \"topRightPane\": [\n      {\"itemProp\": \"address\", \"valueStyle\": \"address\" },\n      {\"itemProp\": \"email\", \"valueStyle\": \"hyperlink\", \"baseUri\": \"mailto:\" }\n    ],\n    \"bottomRightPane\": []\n  }\n</code></pre> <p></p> <p></p>"},{"location":"config/#logo","title":"Logo","text":"<p>The <code>logo</code> config field is a child of the <code>ui</code> field and is used to define logo display and position on the map. The optional <code>smallScreenPosition</code> and <code>largeScreenPosition</code> subfields also allow for fine tuning of the logo position. These values will override the default positioning.</p> <p>If a config file does not contain a <code>logo</code> field, no logo will be displayed on the map.</p> <p>A new folder has been added <code>front-end/public/assets/logo</code> as a central store for logo images.</p> <p>The <code>logo</code> field has 6 subfields</p> <ul> <li><code>largeLogo</code>: the location of the large logo image e.g.: <code>/assets/logos/cwm-logo.png</code></li> <li><code>smallLogo</code>: similar to above, the location of the small logo</li> <li><code>altText</code>: any alt text to be associated with the logo</li> <li><code>smallScreenPosition</code>: position of the logo on small screens, defined with CSS position properties (top, left)</li> <li><code>largeScreenPosition</code>: position of the logo on large screens, defined with CSS position properties (bottom, right)</li> </ul>"},{"location":"config/#example-of-the-logo-config-field-for-cwm","title":"Example of the <code>logo</code> config field for CWM","text":"<pre><code>\"ui\": {\n  \"logo\": {\n    \"largeLogo\": \"./assets/logos/cwm-logo.png\",\n    \"smallLogo\": \"./assets/logos/cwm-logo-small.png\",\n    \"altText\": \"Cooperative World Map\",\n    \"smallScreenPosition\": {\n      \"top\": \"0\",\n      \"left\": \"5px\"\n    },\n    \"largeScreenPosition\": {\n      \"bottom\": \"-25px\",\n      \"right\": \"-5px\"\n    }\n  }\n},\n</code></pre>"},{"location":"config/#map","title":"Map","text":"<p>The <code>map</code> config field is also a child of the <code>ui</code> field and is used to define the boundaries of a map. If a config contains no <code>map</code> field, the whole of the world map will be displayed by default, \u00e0 la the CWM.</p> <p>The <code>map</code> field has 1 subfield</p> <ul> <li><code>mapBounds</code>: an array the countains the longitude and latitude of the map's boundaries</li> </ul>"},{"location":"config/#the-map-config-object-including-the-powys-map-boundaries","title":"The <code>map</code> config object including the Powys map boundaries","text":"<pre><code>\"ui\": {\n  \"map\": {\n    \"mapBounds\": [\n      [-5.5, 51.3],\n      [-2.5, 53.5]\n    ]\n  }\n},\n</code></pre>"},{"location":"config/#the-ui-field-including-the-directory_panel_field-filterablefields-map-and-logo-child-fields","title":"The <code>ui</code> field including the <code>directory_panel_field</code>, <code>filterableFields</code>, <code>map</code> and <code>logo</code> child fields","text":"<pre><code>\"ui\": {\n  \"directory_panel_field\": \"country_id\",\n  \"filterableFields\": [\n    \"country_id\",\n    \"primary_activity\",\n    \"organisational_structure\",\n    \"typology\"\n  ],\n  \"map\": {\n    \"mapBounds\": [\n      [-5.5, 51.3],\n      [-2.5, 53.5]\n    ]\n  },\n  \"logo\": {\n    \"largeLogo\": \"/assets/logos/cwm-logo.png\",\n    \"smallLogo\": \"/assets/logos/cwm-logo-small.png\",\n    \"altText\": \"Cooperative World Map\",\n    \"smallScreenPosition\": {\n      \"top\": \"0\",\n      \"left\": \"5px\"\n    },\n    \"largeScreenPosition\": {\n      \"bottom\": \"-25px\",\n      \"right\": \"-5px\"\n    }\n  }\n},\n</code></pre>"},{"location":"config/#pluralisation","title":"Pluralisation","text":"<p>Pluralisation rules can differ per language, requiring the use of extra suffixes. <code>zero</code>, <code>one</code> and <code>other</code> are sufficient for English, Spanish, French and Hindi, and fine for our current instance of CWM. However, Welsh and Arabic have six categories according to CLDR  and i18next follows this:     - <code>zero</code> (when n = 0)    - <code>one</code> (when n = 10)    - <code>two</code> (when n = 2)    - <code>few</code> (when n= 3)    - <code>many</code> (when n = 6)    - <code>other</code> (everything else)</p> <p>Omission of the required plural forms will cause the translation to fallback to the default language. This tool can be used to determine the necessary plural suffixes, and list of the two letter ISO language codes can be found here. </p>"},{"location":"config/#welsh-ui-translations-including-two-few-and-many","title":"Welsh UI translations including <code>two</code>, <code>few</code> and <code>many</code>","text":"<pre><code>\"ui\": {\n      \"cy\": {\n        \"title\": \"Cyfieithiadau\",\n        \"terms\": {\n          \"directory\": \"Cyfeiriadur\",\n          \"food_system_categories\": \"Categori\",\n          \"primary_food_system_category\": \"Categori\",\n          \"locality\": \"Tref\",\n          \"matching_results_zero\": \"Dim canlyniadau cyfatebol\",\n          \"matching_results_one\": \"{{count}} canlyniad cyfatebol\",\n          \"matching_results_two\": \"{{count}} canlyniadau cyfatebol\",\n          \"matching_results_few\": \"{{count}} canlyniadau cyfatebol\",\n          \"matching_results_many\": \"{{count}} canlyniadau cyfatebol\",\n          \"matching_results_other\": \"{{count}} canlyniadau cyfatebol\",\n          \"directory_entries_zero\": \"Dim cofnodion cyfeiriadur\",\n          \"directory_entries_one\": \"{{count}} cofnod cyfeiriadur\",\n          \"directory_entries_two\": \"{{count}} gofnodion cyfeiriadur\",\n          \"directory_entries_few\": \"{{count}} gofnodion cyfeiriadur\",\n          \"directory_entries_many\": \"{{count}} gofnodion cyfeiriadur\",\n          \"directory_entries_other\": \"{{count}} cofnodion cyfeiriadur\",\n          \"search\": \"Chwilio\",\n          \"clear_search\": \"Clirio'r chwiliad\",\n          \"any\": \"Unrhyw\",\n          \"map\": \"Map\",\n          \"about\": \"Ynghylch\",\n          \"no_location_available\": \"Dim lleoliad ar gael\"\n        }\n      }\n    }\n</code></pre>"},{"location":"data-pipeline/","title":"Mykomap Data","text":"<p>This aims to describe the Mykomap 4.x back-end data format, and the process for generating it.</p>"},{"location":"data-pipeline/#where-is-the-data-stored","title":"Where is the data stored?","text":"<p>Mykomap's data is stored in JSON files below a designated directory on the server.</p> <p>The back-end application needs to be configured with the path to this directory with an entry in <code>.env</code>:</p> <pre><code>SERVER_DATA_ROOT=/some/path\n</code></pre> <p>If this is not set, the default is to use the data in the back-end application's <code>test/data</code> directory - equivalent to setting:</p> <pre><code>SERVER_DATA_ROOT=test/data\n</code></pre> <p>There should be a dataset in there which is used for unit testing. Using this as the default means the back-end has a dataset available out of the box.</p> <p>[!NOTE] The <code>SERVER_DATA_ROOT</code> path can be relative, in which case it is relative to the current working directory of the application. Typically, when run from an <code>npm</code> run-script, this is the root back-end application directory.</p>"},{"location":"data-pipeline/#what-format-is-the-mykomap-data-stored-as","title":"What format is the Mykomap data stored as?","text":"<p>The directory structure looks like this, in schematic form:</p> <pre><code>&lt;dataroot&gt;/\n           &lt;dataset X&gt;/\n                       about.md\n                       config.json\n                       locations.json\n                       searchable.json\n                       items/\n                             0.json\n                             ... &lt;more item files&gt; ...\n                             &lt;N&gt;.json\n           &lt;dataset Y&gt;/\n                       ... &lt;same files as above&gt; ...\n                       items/\n                             ... &lt;item files&gt; ...\n           ... &lt;more datasets&gt; ...\n</code></pre> <p>[!INFO] File names in the schematic above should be interpreted literally - except when enclosed in angle brackets, which means the name is symbolic.</p> <p>A brief synopsis of these files' structure and purpose:</p> <ul> <li><code>about.md</code>: A human readable description of the dataset, in markdown   format. It gets shown in the \"about\" side-bar panel of the map.</li> <li><code>config.json</code>: Meta-data for the dataset, such as property schema,   vocabulary definitions, and map configuration parameters goes here.</li> <li><code>locations.json</code>: An ordered list of longitude/latitude locations,   one per data item.</li> <li><code>searchable.json</code>: An ordered index of the searchable and filterable   item properties.</li> <li><code>items/*.json</code>: These files store the full information for each data   item.</li> </ul> <p>More details of these files can be found in the section below, under More Details.</p>"},{"location":"data-pipeline/#why-is-the-format-like-this","title":"Why is the format like this?","text":"<p>The aims of this format's design are to:</p> <ul> <li>Support very large dataset sizes, of approaching 500 thousand items.</li> <li>Allow maximally fast responses to browsers loading and querying   these datasets via the API, by storing data in a form identical to   that being served to API clients.</li> <li>Leverage JSON where possible. This format has highly optimised   native support on browser platforms, whereas other formats such as   CSV usually cannot compete because they need to be parsed in   JavaScript - this usually results in more memory use and slower   parsing speeds.</li> <li>Avoid the need for a relational or triple-store database, which   comes with some overhead, both in resources and cognition.</li> <li>Minimise the verbosity of data so far as possible, such that file   sizes are not too large.</li> <li>\"Keep It Simple\" so far as possible, we want a format which is easy   to understand and work with - when it does not affect the above   goals.</li> <li>Build on the concepts and precedents of earlier Mykomap data. For   instance, we utilise SKOS vocabularies and [QNames][qnames],   and we use more or less the same property definition and vocabulary   schema.</li> </ul> <p>In the first instance, when a map is loaded, we want to minimise the delay between opening a map link and the user seeing pins appear on the map. With a large number of pins, some sort of clustering of item locations is needed, and the clustering algorithm contributes the largest amount to that delay.</p> <p>We use [MapLibre][maplibre] to do this clustering, which uses a well-optimised algorithm. This library is based on the open source version of the commercial [MapBox][mapbox] library, and is much faster than [Leaflet][leaflet] with the Leaflet.markercluster plug-in that was used for earlier versions of Mykomap. Nevertheless, it can take a small number of seconds to load a file with half a million locations and then cluster them, yet alone when extra data is added.</p> <p>As main information needed up-front when loading a map are the pin locations, these are stored by themselves in a file <code>locations.json</code> as a minimised JSON array of coordinate pairs, without white-space and with a restricted number of decimal places. This can be sent verbatim as soon as the dataset ID is known.</p> <p>Item IDs are omitted from this <code>locations.json</code> file, which further minimises the size of the data. Instead of using IDs, dataset items are loaded in a predefined order and then identified within the context of the dataset by their index number. This has some consequences, however, described later.</p> <p>As most of the other information about the items will not be needed until a user starts to browse the pins on the map, loading this information is deferred. The full information for each dataset item is stored in a separate file for it, with a name constructed from the item index number (not the item ID - a consequence of the locations not having IDs included). JSON is also used here, but since these files are relatively small, they are indented for readability. A dataset item's ID can be found by looking in these files.</p> <p>After the locations, something which is also needed early on loading a map,</p> <p>Although typically much smaller and less urgently required than locations data, what's needed next when loading a map are the following:</p> <ul> <li>Vocabulary definitions, which is shown in the side-bar filters;</li> <li>The item property schema, which dictates how items should be   interpreted, searched, and displayed; and</li> <li>The map configuration settings, which dictate how the map is rendered.</li> </ul> <p>These are all stored as JSON and indented for readability in <code>config.json</code>.</p> <p>Finally, there needs to be an index for filtering by category and text searches. This is stored as JSON in <code>searchable.json</code>.</p> <p>As this file can be a lot of items, this file can become quite large and we need to be parsimonious with formatting characters. Therefore is not indented, and uses arrays rather than objects, to avoid repeating the keys for every item. However, for the sake of human readability and inspection, it is broken up such that headers are defined on the first line, and each subsequent line is data for one item, in the defined order. The result reads somewhat like a CSV file.</p>"},{"location":"data-pipeline/#how-to-create-a-dataset","title":"How to create a dataset.","text":"<p>Typically, the data for datasets is supplied in some format defined by the supplier. Common examples are spreadsheets; CSV or TSV files; and JSON, possibly via some API over the web; or in some cases sent manually by email.</p> <p>In addition to the data itself, we need definitions of the semantics and syntax within the data - such as the taxonomies, any mark-up or encoding schemes, and identifiers used. These are typically sent by email... or, in the worst case, we must try to infer them from the data.</p> <p>It is a requirement that the data is logically tabular - a sequence of items sharing the same set of properties - and that each dataset item representing an entry in the map or directory has a unique identifier within the dataset.</p> <p>We then need to transform this into the prerequisites for the <code>dataset import</code> tool, which generates Mykomap datasets.</p>"},{"location":"data-pipeline/#prerequisites-and-process","title":"Prerequisites and process","text":"<p>There are two specific files needed as prerequisites for <code>dataset import</code>.</p> <ul> <li>A CSV file with a compatible suitable schema, including a row for   each dataset item.</li> <li>A <code>config.json</code> file containing all the required metadata.</li> </ul> <p>More about these below, but the latter is typically the same one used in the dataset - in fact the <code>dataset</code> tool copies it into place in the output folder.</p> <p>The output is a new folder, containing the dataset files. The name of this folder is interpreted as the (URI-component encoded) dataset ID when it is written or copied into the root data directory (set by <code>SERVER_DATA_ROOT</code> in the back-end's <code>.env</code> file.)</p> <p>[!INFO] The <code>dataset</code> command will not write the data if the output directory already exists. It is up to you to delete a directory first if you want to overwrite it - this to make it harder to accidentally overwrite something else with the data.</p> <p>The <code>dataset</code> tool is part of the <code>back-end</code> application, and can be run from the <code>apps/back-end</code> project directory using <code>npm</code>. The commands supports various subcommands, and the relevant one for importing data is <code>import</code>.</p> <p>It can be used like this:</p> <pre><code>npm run dataset import $CONFIG $CSV $OUTPUT_DIR\n</code></pre> <p>Where:</p> <ul> <li><code>$CONFIG</code> should be the path to the <code>config.json</code> file,</li> <li><code>$CSV</code> should be the path to the CSV file,</li> <li><code>$OUTPUT_DIR</code> should be the path to write the output dataset files to.</li> </ul> <p>[!INFO] More details can be obtained by running the command</p> <pre><code>npm run dataset -- import --help\n</code></pre>"},{"location":"data-pipeline/#configjson","title":"<code>config.json</code>","text":"<p>The full schema of this file is outside the scope of this document, for the sake of keeping it fairly short. It's also likely to evolve from the exact schema at the time of writing.</p> <p>For now I will just note that some of the information needed for <code>config.json</code> must be composed manually; and some, those parts describing the vocabularies, can be obtained from the <code>open-data</code> pipeline's output file, <code>vocabs.json</code>.</p> <p>[!NOTE] Beware that the name of this file is configurable and may vary from case to case, although the default is <code>vocabs.json</code>.</p> <p>[!INFO] The format of <code>vocabs.json</code> files are very similar (if historically not quite identical) to <code>config.json</code>'s <code>vocabs</code> attribute. The information is composed from one or more existing SKOS vocabularies.</p> <p>Another important function of the <code>config.json</code> file is to describe the properties of the dataset items, which is done in the <code>itemProps</code> attribute.</p> <p>And finally, these descriptions include an attribute <code>from</code> which identifies which CSV header to source the property value from. This is for the benefit of the <code>dataset</code> command.</p>"},{"location":"data-pipeline/#the-csv-file","title":"The CSV file","text":"<p>The format of this file is essentially dictated by <code>config.json</code>'s <code>itemProps.&lt;item&gt;.from</code> attribute, as described above. The CSV may have more headers than this - these will be ignored.</p> <p>CSV fields are interpreted depending on the other attributes of the <code>itemProps</code> property descriptions. See [More Details][#more-details] below.</p>"},{"location":"data-pipeline/#standardcsv","title":"<code>standard.csv</code>","text":"<p>Each of the datasets in the open-data project output a CSV file containing the data in a normalised form, with added or refined information (typically geocoded locations). This is typically what we use as input to the <code>dataset import</code> command.</p> <p>This CSV file has a set of standard headers. It can also have any number of arbitrary optional extras. For historical reasons this schema is called the \"Standard CSV format\", and the file is called <code>standard.csv</code></p> <p>Optionally RDF data can also be generated by open-data datasets, but this is not useful for <code>dataset import</code>.</p> <p>[!INFO] This step is performed by code in the open-data and se-open-data projects, and typically published on http://data.digitalcommons.coop/</p> <p>In the current incarnation of the schema, the standard headers are:</p> <ul> <li>Identifier</li> <li>Name</li> <li>Description</li> <li>Organisational Structure</li> <li>Primary Activity</li> <li>Activities</li> <li>Street Address</li> <li>Locality</li> <li>Region</li> <li>Postcode</li> <li>Country ID</li> <li>Territory ID</li> <li>Website</li> <li>Phone</li> <li>Email</li> <li>Twitter</li> <li>Facebook</li> <li>Companies House Number</li> <li>Qualifiers</li> <li>Membership Type</li> <li>Latitude</li> <li>Longitude</li> <li>Geo Container</li> <li>Geo Container Confidence</li> <li>Geo Container Latitude</li> <li>Geo Container Longitude</li> <li>Geocoded Address</li> </ul> <p>However, any given dataset can have arbitrary additional headers added when convenient.</p> <p>[!INFO] More of the gory details of the semantics are embedded as comments in the code and data here.</p> <p>Many of these headers are in fact somewhat specific to the ICA and Co-ops UK datasets, and often left blank. The key ones for most purposes are:</p> <ul> <li>Identifier: an arbitrary but unique (to the dataset) string.</li> <li>Name: an arbitrary short string; displayed as pin names.</li> <li>Street Address: an arbitrary string which can be combined into an   address for geocoding, if necessary, and/or displayed on pin pop-ups.</li> <li>Locality: ditto; combined into the address.</li> <li>Region: ditto; combined into the address.</li> <li>Postcode: ditto; combined into the address, and/or used as for geocoding.</li> <li>Country ID: ditto; combined into the address, and/or used as a filter category.</li> <li>Latitude: a decimal number; indicates a manually supplied location.</li> <li>Longitude: a decimal number; indicates a manually supplied location.</li> <li>Geo Container: a unique URI representing the location; typically   an open-street-map location URL.</li> <li>Geo Container Confidence: a decimal; interpreted as a percentage   that indicates the geocoding confidence.</li> <li>Geo Container Latitude: a decimal number; indicates a geocoded   location.</li> <li>Geo Container Longitude: a decimal number; indicates a geocoded location.</li> <li>Geocoded Address: an arbitrary string which records the (typically   cleaned up) address which was actually passed to the geocoder; used   for diagnosing problems with geocoded locations.</li> </ul> <p>The only strictly mandatory one is Identifier. Location coordinates of some sort is obviously needed for a marker to appear on the map, and a Name is needed for it to be identifiable.</p>"},{"location":"data-pipeline/#example","title":"Example","text":"<p>Here's a specific example of the process just described. It's based on the dataset currently in the back-end test data.</p> <p>First, the CSV data:</p> <pre><code>Identifier,Name,Desc,Address,Websites,Activity,Other Activities,Latitude,Longitude,Geocoded Latitude,Geocoded Longitude,Validated\naaa,\"Apple Co-op\",We grow fruit.\",\"1 Apple Way, Appleton\",http://apple.coop,AM130,,0,0,51.6084367,-3.6547778,true\nbbb,\"Banana Co\",\"We straighten bananas.\",\"1 Banana Boulevard, Skinningdale\",http://banana.com,AM60,AM70;AM120,0,0,55.9646979,-3.1733052,false\nccc,\"The Cabbage Collective\",\"We are artists.\",\"2 Cabbage Close, Caulfield\",http://cabbage.coop;http://cabbage.com,AM60,AM130,,,54.9744687,-1.6108945,true\nddd,\"The Chateau\",\"The Chateau is not a place, it is a state of mind.\",,,,,,,,,\n</code></pre> <p>Or in a more readable table format:</p> Identifier Name Desc Address Websites Activity Other Activities Latitude Longitude Geocoded Latitude Geocoded Longitude Validated aaa Apple Co-op We grow fruit. 1 Apple Way, Appleton http://apple.coop AM130 0 0 51.6084367 -3.6547778 true bbb Banana Co We straighten bananas. 1 Banana Boulevard, Skinningdale http://banana.com AM60 AM70;AM120 0 0 55.9646979 -3.1733052 false ccc The Cabbage Collective We are artists. 2 Cabbage Close, Caulfield http://cabbage.coop;http://cabbage.com AM60 AM130 54.9744687 -1.6108945 true ddd The Chateau The Chateau is not a place, it is a state of mind. <p>Now, the <code>config.json</code>. We can bend the strict JSON format here to use comments to annotate it, but in the real one comments would upset the JSON parser, so omit them.</p> <pre><code>{\n  \"prefixes\": {\n    // There's just one prefix defined here, because just one vocabulary\n    \"https://dev.lod.coop/essglobal/2.1/standard/activities-modified/\": \"am\"\n  },\n  \"languages\": [\"en\"], // Only one language supported: English\n  \"ui\": { \"directory_panel_field\": \"activity\" }, // Sets the default panel to show\n  \"vocabs\": {\n    \"am\": { // This is the Activities vocabulary\n      \"en\": { // The English title and term definitions follow\n        \"title\": \"Activities (Modified)\",\n        \"terms\": {\n          \"AM10\": \"Arts, Media, Culture &amp; Leisure\",\n          \"AM20\": \"Campaigning, Activism &amp; Advocacy\",\n          \"AM30\": \"Community &amp; Collective Spaces\",\n          \"AM40\": \"Education\",\n          \"AM50\": \"Energy\",\n          \"AM60\": \"Food\",\n          \"AM70\": \"Goods &amp; Services\",\n          \"AM80\": \"Health, Social Care &amp; Wellbeing\",\n          \"AM90\": \"Housing\",\n          \"AM100\": \"Money &amp; Finance\",\n          \"AM110\": \"Nature, Conservation &amp; Environment\",\n          \"AM120\": \"Reduce, Reuse, Repair &amp; Recycle\",\n          \"AM130\": \"Agriculture\",\n          \"AM140\": \"Industry\",\n          \"AM150\": \"Utilities\",\n          \"AM160\": \"Transport\"\n        }\n      }\n      // ... similar definitions for other languages would go here,\n      // using the same structure.\n    }\n  },\n  \"itemProps\": { // This defines the properties dataset items should have\n    \"id\": { // A unique identifier. Mandatory for the dataset to be usable.\n      \"type\": \"value\",      // Single valued\n      \"from\": \"Identifier\", // Which CSV field to read it from\n      \"strict\": true        // Don't tolerate nulls or blanks.\n                            // Format is \"string\" by default.\n    },\n    \"name\": { // The name of the item\n      \"type\": \"value\",\n      \"search\": true, // we want this to be text-searchable\n      \"from\": \"Name\"\n    },\n    \"manlat\": {\n      \"type\": \"value\",\n      \"as\": \"number\",    // Convert these values into numbers when loading CSV\n      \"nullable\": true,  // Allow nulls\n      \"from\": \"Latitude\"\n    },\n    \"manlng\": {\n      \"type\": \"value\",\n      \"as\": \"number\",\n      \"nullable\": true,\n      \"from\": \"Longitude\"\n    },\n    \"lat\": {\n      \"type\": \"value\",\n      \"as\": \"number\",\n      \"nullable\": true,\n      \"from\": \"Geocoded Latitude\"\n    },\n    \"lng\": {\n      \"type\": \"value\",\n      \"as\": \"number\",\n      \"nullable\": true,\n      \"from\": \"Geocoded Longitude\"\n    },\n    \"address\": {\n      \"type\": \"value\",\n      \"filter\": true,\n      \"from\": \"Address\"\n    },\n    \"activity\": {\n      \"type\": \"vocab\",   // A single-valued identifier from a vocabulary\n      \"uri\": \"am:\",      // Specifically, this vocabulary\n      \"from\": \"Activity\"\n    },\n    \"otherActivities\": {\n      \"type\": \"multi\",   // Multi-valued\n      \"of\": {\n        \"type\": \"vocab\", // Values are identifiers from a vocabulary\n        \"uri\": \"am:\"     // Specifically, this one\n      },\n      \"from\": \"Other Activities\" // When interpreting CSV, by default a\n                                 // semi-colon is used as a delimiter.\n                                 // Literal semi-colons need to be escaped\n                                 // with a backslash.\n    },\n    \"websites\": {\n      \"type\": \"multi\",   // Multi-valued\n      \"of\": {\n        \"type\": \"value\"  // Values are strings\n      },\n      \"from\": \"Websites\"\n    },\n    \"validated\": {\n      \"type\": \"value\",    // Single valued\n      \"as\": \"boolean\",    // Convert values to booleans, or null if they're too \"weird\".\n                          // Non-weird means: \"true\", \"yes\", \"y\", \"t\" or \"1\" count as true,\n                          // and \"false\", \"no\", \"n\", \"f\" or \"0\" as false. (Ignoring case\n                          // in both cases.)\n      \"from\": \"Validated\"\n    }\n  }\n}\n</code></pre> <p>Given these files as inputs, <code>data.csv</code> and <code>config.json</code>, and assuming:</p> <ul> <li>these files are in <code>/tmp/</code>, which is writable</li> <li>there is no dierctory <code>/tmp/out</code> existing</li> <li>the current directory is <code>apps/back-end/</code></li> </ul> <p>...we can then run the following to generate a dataset in <code>/tmp/out/</code>:</p> <pre><code>npm run dataset import /tmp/config.json /tmp/data.csv /tmp/out\n</code></pre> <p>We should then get a dataset written to <code>/tmp/out/</code>.</p> <p>Listing that would get:</p> <pre><code>$ find /tmp/out -type f\n\n /tmp/out/config.json\n /tmp/out/items\n /tmp/out/items/0.json\n /tmp/out/items/1.json\n /tmp/out/items/2.json\n /tmp/out/items/3.json\n /tmp/out/locations.json\n /tmp/out/searchable.json\n</code></pre> <p>...where <code>config.json</code> should be identical to the above. The other files' contents would be as below....</p>"},{"location":"data-pipeline/#locationsjson","title":"<code>locations.json</code>","text":"<pre><code>[[-3.65478,51.60844],[-3.17331,55.9647],[-1.61089,54.97447],null]\n</code></pre>"},{"location":"data-pipeline/#searchablejson","title":"<code>searchable.json</code>","text":"<pre><code>{   \"itemProps\":\n[\"name\",\"address\",\"searchString\"],\n    \"values\":[\n[\"Apple Co-op\",\"1 Apple Way, Appleton\",\"apple co op\"],\n[\"Banana Co\",\"1 Banana Boulevard, Skinningdale\",\"banana co\"],\n[\"The Cabbage Collective\",\"2 Cabbage Close, Caulfield\",\"the cabbage collective\"],\n[\"The Chateau\",\"\",\"the chateau\"]\n]}\n</code></pre>"},{"location":"data-pipeline/#items0json","title":"<code>items/0.json</code>","text":"<pre><code>{\n  \"id\": \"aaa\",\n  \"name\": \"Apple Co-op\",\n  \"manlat\": 0,\n  \"manlng\": 0,\n  \"lat\": 51.60844,\n  \"lng\": -3.65478,\n  \"address\": \"1 Apple Way, Appleton\",\n  \"activity\": \"AM130\",\n  \"otherActivities\": [],\n  \"websites\": [\n    \"http://apple.coop\"\n  ],\n  \"validated\": true\n}\n</code></pre>"},{"location":"data-pipeline/#items1json","title":"<code>items/1.json</code>","text":"<pre><code>{\n  \"id\": \"bbb\",\n  \"name\": \"Banana Co\",\n  \"manlat\": 0,\n  \"manlng\": 0,\n  \"lat\": 55.9647,\n  \"lng\": -3.17331,\n  \"address\": \"1 Banana Boulevard, Skinningdale\",\n  \"activity\": \"AM60\",\n  \"otherActivities\": [\n    \"AM70\",\n    \"AM120\"\n  ],\n  \"websites\": [\n    \"http://banana.com\"\n  ],\n  \"validated\": false\n}\n</code></pre>"},{"location":"data-pipeline/#items2json","title":"<code>items/2.json</code>","text":"<pre><code>{\n  \"id\": \"ccc\",\n  \"name\": \"The Cabbage Collective\",\n  \"manlat\": null,\n  \"manlng\": null,\n  \"lat\": 54.97447,\n  \"lng\": -1.61089,\n  \"address\": \"2 Cabbage Close, Caulfield\",\n  \"activity\": \"AM60\",\n  \"otherActivities\": [\n    \"AM130\"\n  ],\n  \"websites\": [\n    \"http://cabbage.coop\",\n    \"http://cabbage.com\"\n  ],\n  \"validated\": true\n}\n</code></pre>"},{"location":"data-pipeline/#items3json","title":"<code>items/3.json</code>","text":"<pre><code>{\n  \"id\": \"ddd\",\n  \"name\": \"The Chateau\",\n  \"manlat\": null,\n  \"manlng\": null,\n  \"lat\": null,\n  \"lng\": null,\n  \"address\": \"\",\n  \"activity\": null,\n  \"otherActivities\": [],\n  \"websites\": [],\n  \"validated\": false\n}\n</code></pre>"},{"location":"data-pipeline/#more-details","title":"More details","text":"<p>Here we go into more detail about the dataset files and their structure.</p>"},{"location":"data-pipeline/#datasets","title":"Datasets","text":"<p>At the top level below <code>&lt;dataroot&gt;</code>, which is whatever <code>SERVER_DATA_ROOT</code> has been configured with, there are zero or more directories: one for each dataset. All the data for a dataset is contained within its dataset directory.</p> <p>The names of the directories are intepreted as the dataset IDs, URI-component encoded to ensure that they are well-formed directory names: characters like \"/\" are encoded as \"%2F\", for instance.</p> <p>[!NOTE] The data inside a dataset directory does not refer to its own ID, so the directory can be renamed freely without breaking the data structure within.</p> <p>Inside these top-level directories are a number of mandatory data files, and a sub-directory named <code>items</code> which contains a data file for each of the dataset's items.</p>"},{"location":"data-pipeline/#aboutmd-mandatory","title":"<code>about.md</code> (mandatory)","text":"<p>This file contains a free-form human-readable description of the dataset in Markdown format.</p> <p>It will be shown in the \"about\" panel of the map when this dataset is selected. You can put anything you like in here, but you should keep this viewing context in mind.</p> <p>[!WARNING] Currently there is no localisation support for this file, this should be added in future.</p>"},{"location":"data-pipeline/#configjson-mandatory","title":"<code>config.json</code> (mandatory)","text":"<p>This is a JSON data file that describes:</p> <ul> <li>the properties shared by dataset item, and the mapping to this from   CSV columns,</li> <li>vocabularies and localisations thereof used in the data (AKA taxonomies),</li> <li>and map configuration details.</li> </ul> <p>The schema and semantics of this file is more complex than the other files. The formal specification is defined by the <code>ConfigData</code> type contract in <code>contract.ts</code> - refer to that for the full details of nested properties.</p> <p>Here we list the top-level elements and their purpose.</p> <p>[!WARNING] Although correct at the time of writing, this is subject to change.</p> <ul> <li><code>prefixes</code>: An object mapping SKOS vocabulary URIs to the   abbreviations used in [QNames][qnames] (\"qualified names\") in the   rest of the file. QNames are a mechanism used in both   RDF/XML and Turtle file formats for encoding   RDF linked data</li> <li><code>languages</code>: An array of two-character ISO-639-1 codes.   It enumerates the languages that are supported by the vocabularies,   defined in the <code>vocabs</code> property. There should be at least one   element. The first element is assumed to be the default language to use.</li> <li><code>ui</code>: A collection of user-interface settings for the map when   showing this dataset. For example: the side panel to show by   default. These are likely to change, and not strictly necessary for   generating a dataset.</li> <li> <p><code>itemProps</code>: An object defining the properties required for each   data item. The keys define the property IDs, and the values their   definitions, which are sub-objects. A non-exhaustive list of the   most important properties of dataset-item property definitions are:</p> </li> <li> <p><code>type</code>: Required, indicates the basic property type, and can be one of the following:</p> <ul> <li><code>\"value\"</code>, this property represents a single literal strings;</li> <li><code>\"vocab\"</code>, this property represents a single identifier from a   specific vocabulary defined in this file;</li> <li><code>\"multi\"</code>, this property represents a multi-valued property,   containing instances of either <code>vocab</code> or <code>value</code>.</li> </ul> </li> <li><code>of</code>: Required for <code>\"multi\"</code> properties - defines the type of     the multiple values. Can be <code>\"value\"</code> or <code>\"vocab\"</code>.</li> <li><code>filter</code>: Optional, indicates if the property should be     filterable; boolean, defaults to false.</li> <li><code>search</code>: Optional, indicates if the property should be     text-searchable; boolean, defaults to false.</li> <li><code>from</code>: Optional, used when importing datasets from CSV. It     indicates the name of a CSV column header to use as the source for     this field. If absent, this field will not get populated by the importer.</li> <li><code>nullable</code>: Optional, indicates whether null values are permitted     in the data without triggering an error. (Note that for CSV data,     an empty field counts as null.) Boolean, defaults to true.</li> <li><code>strict</code>: Optional, indicates whether to be sloppy or strict when interpreting CSV data.</li> <li> <p><code>as</code>: Optional, hints on the interpretation of data when converting from CSV.     Can be one of:</p> <ul> <li><code>\"string\"</code>: interpret as a string.</li> <li><code>\"boolean\"</code>: interpret as a boolean.</li> <li><code>\"number\"</code>: interpret as a number.</li> </ul> </li> <li> <p><code>vocabs</code>: An object defining the vocabularies used in the vocab   properties of dataset items. Keys are vocabulary abbreviations as   defined in <code>prefixes</code>. They map to a vocabulary definition for each   language - keyed by a language code defined in <code>languages</code>. Each   vocabulary definition is in the context of that language, and has a   title for the vocabulary and a mapping from term IDs to term labels   for that language. All this is to allow identifiers representing   vocabulary terms (typically IDs, [QNames][qnames], or full URIs)   into localised labels. This useful both for interpreting dataset item   properties, but also localisation of the user interface.</p> </li> </ul>"},{"location":"data-pipeline/#locationsjson-mandatory","title":"<code>locations.json</code> (mandatory)","text":"<p>This is a JSON data file that contains an array of locations, one for each item in the dataset.</p> <p>The order of this array is significant. The Nth item in the locations array represents the location of the Nth item in the dataset. There must be one item in the array for each item in the dataset.</p> <p>Each location is represented by an array of two floating point coordinates:</p> <pre><code>[&lt;longitude&gt;, &lt;latitude&gt;],\n</code></pre> <p>Or, if there is no location for that item, just a null as a placeholder:</p> <pre><code>null,\n</code></pre> <p>[!NOTE] Typically, the coordinates stored in the array only use just enough decimal places to identify a street address on the map uniquely throughout the world and no more: five. This is avoid the file from becoming unnecessarily big; JSON floating point representations can be quite long.</p> <p>For similar reasons, all white-space and newlines are omitted. This will make human inspection of very large files somewhat awkward but this is considered worth the inconvenience. If necessary the file can be piped through a tool like <code>jq</code> or <code>sed</code> to unpack it for inspection.</p>"},{"location":"data-pipeline/#searchablejson_1","title":"<code>searchable.json</code>","text":"<p>This is a JSON data file that contains an index of the searchable properties of the dataset items.</p> <p>The formal specification of this file, such that it exists, is defined in the source code currently.</p> <p>In brief: it is a JSON object with two properties:</p> <ul> <li><code>itemProps</code>, a list of property IDs, and</li> <li><code>values</code>, a list of property value arrays.</li> </ul> <p>Like this:</p> <pre><code>{\n  \"itemProps\": [\n    ...\n  ],\n  \"values\": [\n    ...\n  ]\n}\n</code></pre> <p>[!INFO] Note, this example is indented for readability, but the actual file is somewhat minimised and omits superfluous white-space. <code>itemProps</code> is inserted on the first line, and subsequent lines define items in the values array, one per line in the predefined order.</p>"},{"location":"data-pipeline/#itemprops","title":"<code>itemProps</code>","text":"<p><code>itemProps</code> defines how to interpret the value arrays. For example:</p> <pre><code>\"itemProps\": [\n  \"country_id\",\n  \"primary_activity\",\n  \"typology\",\n  \"data_sources\",\n  \"searchString\"\n],\n...\n</code></pre> <p>The elements refer to the property IDs defined in <code>config.json</code>.</p> <p>In this case, this indicates that there are four properties which can be searched by drop-down filter.</p> <p>The fifth element <code>searchString</code> is special: it's not a real property of the dataset item, but synthesised from them; and is always present (if only as an empty string). It represents the text-searchable content of the dataset item, normalised to punctuation-free lower case words.</p>"},{"location":"data-pipeline/#values","title":"<code>values</code>","text":"<p>The <code>values</code> array must contain one element for each of the dataset items.</p> <p>The order of the <code>values</code> array is significant: the Nth item of the <code>values</code> array corresponds to the Nth dataset item.</p> <p>Each element of <code>values</code> is an array of property values the same size as the <code>itemProps</code> array. For instance:</p> <pre><code>...\n\"values\": [\n  [\n    \"GB\",\n    \"ICA210\",\n    \"BMT20\",\n    [\"DC\", \"CUK\"],\n    \"housing 1 west street sheffield s1 2ab uk apples coop\"\n  ],\n  ...\n\n]\n</code></pre> <p>In this example, the first dataset item has a <code>country_id</code> of <code>\"GB\"</code>, a <code>primary_activity</code> of <code>\"ICA212\"</code>, and so on.</p> <p>The <code>searchString</code> text is the last element.</p>"},{"location":"data-pipeline/#searchstring-construction","title":"<code>searchString</code> construction","text":"<p>To illustrate where <code>searchString</code> comes from, consider an example case where text-searchable properties have been configured in <code>config.json</code> to be:</p> <ul> <li><code>activities</code>,</li> <li><code>address</code> and</li> <li><code>name</code>.</li> </ul> <p>These must be valid dataset item properties of course, but don't necessarily need to be properties listed in <code>itemProps</code>.</p> <p>Let's say the values of those properties are:</p> <ul> <li><code>\"ICA210\"</code> - an activity ID which would be expanded to \"Housing\" in English;</li> <li><code>\"1 West Street, Sheffield S1 2AB, UK\"</code> (an address), and</li> <li><code>\"Apples Co-op\"</code> (the organisation name).</li> </ul> <p>The <code>searchString</code> value will be those values concatenated, with punctuation removed and text down-cased, like this:</p> <p>housing 1 west street sheffield s1 2ab uk apples coop</p> <p>This means that a search for \"housing\" or \"west street\" will match.</p> <p>[!WARNING] This illustration assumes vocab term ID are expanded into English, which a) may not be implemented yet, and b) obviously won't support multi-lingual text searches. This may be addressed in future. Also, the type of matching for multiple words may be adjusted.</p>"},{"location":"data-pipeline/#items","title":"<code>items/</code>","text":"<p>This directory contains one file for each dataset item. The files are named according to the item index, with a <code>.json</code> suffix. Their base names are decimal integers, starting with zero. Therefore <code>0.json</code> is the file for the first dataset item, <code>1.json</code> the second, and so on.</p> <p>Each file contains a JSON object mapping all of the defined dataset item property IDs to values.</p> <p>There must be an <code>id</code> property, which is the ID of the item, whose format is determined by the dataset - it must be unique to the dataset.</p> <p>There should be a <code>name</code> property, which is a short human-readable name for the item. This is not mandated, but is advisable.</p> <p>And there should be <code>latitude</code> and <code>longitude</code> properties, which are decimal latitude/longitude coordinates of the item, if it has one. If absent, then the item will be included in search results, but will not have a pin on the map.</p> <p>These and all other fields must be as defined by the <code>itemProps</code> field in <code>config.json</code>.</p> <p>Absent values should be <code>null</code>, or <code>[]</code> if the property is a multi-valued property.</p> <p>Vocabulary properties should be a bare valid vocab term identifier, without any prefix or URI.</p> <p>For example:</p> <pre><code>{\n  \"id\": \"test/cuk/R000001\",\n  \"name\": \"Apples Co-op\",\n  \"description\": \"We sell apples\",\n  \"website\": [\"https://apples.coop\", \"https://orchards.coop\"],\n  \"dc_domains\": [\"apples.coop\", \"orchards.coop\"],\n  \"country_id\": \"GB\",\n  \"primary_activity\": \"ICA210\",\n  \"organisational_structure\": \"OS60\",\n  \"typology\": \"BMT20\",\n  \"latitude\": 51.507476,\n  \"longitude\": -0.127825,\n  \"geocontainer_lat\": 51.50747643,\n  \"geocontainer_lon\": -0.12782464,\n  \"address\": \"1 West Street, Sheffield, S1 2AB, UK\",\n  \"data_sources\": [\"DC\", \"CUK\"]\n}\n</code></pre>"},{"location":"data-pipeline/#glossary","title":"Glossary","text":"<ul> <li>category: when used here, loosely means the same as vocabulary.</li> <li>CSV: \"comma-separated values\" - a common format for storing   tabular data in text files. See Wikipedia. Or CSV files begin   with a line which contain the headers which define names for the   fields found in the following rows. Note that a CSV row may span   more than one line, as it can contain embedded newlines.</li> <li>data root: the root directory in which to find Mykomap dataset files.</li> <li>dataset: a collection of information for use in a Mykomap;   typically consists of an ordered collection of items with some metadata   describing the properties and types of those properties.</li> <li>dataset ID: a unique identifier for a dataset.</li> <li>dataset item: represents a single entry in a dataset, typically   representing an organisation, usually with an associated location   and various other properties.</li> <li>dataset item ID: a unique identifier for a dataset item.</li> <li>dataset item index: a unique zero-based index number for a dataset   item.</li> <li>dataset item property: a named attribute of a dataset item, which   can have one of a number of pre-defined values and types.</li> <li>field: one of the components of a data record, such as in a CSV   file or a database table. Often used to refer to a JSON or   JavaScript property. We try to consistently use it to mean   elements of a CSV row in the context of Mykomap.</li> <li>property: one of the components of a JSON or JavaScript object;   sometimes also referred to as a field or an attribute. We try   and refer consistently to properties in the context of Mykomap.</li> <li>QName: a scheme for representing URIs in a more succinct form within   contexts where brevity and readability is useful. See Wikipedia for more information.</li> <li>RDF: \"Resource Description Format\": a formal scheme for representing semantic descriptions of concepts and their relationships as subject-predicate-object triplets in a machine-readable way. See Wikipedia.</li> <li>RDF/XML: a file encoding for RDF based on XML. See Wikipedia.</li> <li>SKOS: a linked-data standard for describing a collection of terms,   representing concepts, identified by a URI. Terms are also   identified by URIs sharing the same stem as the vocabulary's URI,   followed by a unique identifier. The vocabulary has a localised   title, and each term can have one or more localised labels. More   properties can be ascribed to terms and the vocabulary, but see the   definition on Wikipedia for more details.</li> <li>taxonomy: when used here, loosely means the same as vocabulary.</li> <li>Turtle: a more succinct and readable file encoding for RDF than RDF/XML. See Wikipedia.</li> <li>vocabulary: a set of pre-defined labels for some set of concepts,   in this context usually a SKOS vocabulary.</li> <li>vocabulary term: a concept from a vocabulary.</li> <li>vocabulary URI: a unique identifier and namespace for a vocabulary.</li> <li>vocabulary prefix: a short locally-defined identifier representing   a full URI for the sake of brevity - the first half of a QName</li> </ul>"},{"location":"deployment/","title":"Deploying","text":"<p>Conceptually, installation of these applications require:</p> <ul> <li>deploying the front-end as content to be served on the web</li> <li>configure it to use the correct path for the back end (typically <code>/api</code>)</li> <li>also configure API keys for GlitchTip and MapTiler services</li> <li>deploying the back end to be run as a persistent node process</li> <li>configure it to use the right path to the data folder</li> <li>deploy the data in the path expected by the back-end</li> <li>reverse-proxying the back end to be accessible on the same domain as the front end</li> </ul> <p>This document describes the required steps to achieve the above, either for a first time install or an update.</p>"},{"location":"deployment/#dcc-server-specifics","title":"DCC Server specifics","text":"<p>Note: Although the application could be deployed in various scenarios in principle, this is the only case we specifically cater for now.</p>"},{"location":"deployment/#what-a-dcc-server-provides","title":"What a DCC Server provides","text":"<p>The parts relevant here are, broadly:</p> <ul> <li>A Ubuntu Linux server.</li> <li>With the Apache webserver.</li> <li>The NodeJS runtime (using ASDF, the version of NodeJS can be   specified in the app with <code>.tool-versions</code>)</li> <li>User-mode SystemD services.</li> <li>A dedicated user for the application.</li> </ul> <p>Without going into too much detail, this situation is currently set up via some Ansible playbooks in the DCC technology-and-infrastructure project.</p> <p>The following instructions following assume this context.</p>"},{"location":"deployment/#definitions-for-convenience","title":"Definitions, for convenience","text":"<p>For the descriptions below, we use the following placeholders for generality. (We write them in the style of environment variables, but you can equally see them as just labels.)</p> <ul> <li><code>$SERVER</code> is the ssh URI used for the hostname being deployed to.</li> <li><code>$USER</code> is the user being deployed to on that host.</li> <li><code>$GIT_WORKING</code> is the directory where <code>mykomap-monolith</code> repository   is checked out (or if not checked out, unpacked - in which case   replace <code>git clone</code> or <code>git pull</code> with an appropriate unpacking   process)</li> <li><code>$DATA_DIR</code> is the path to the directory containing data for the back-end.</li> <li>A file exists at <code>$DEPLOY_ENV</code> defining the environment variables   specifically needed for deployment.</li> </ul> <p>Examples, at the time of writing, of the typical case for these are:</p> <pre><code>SERVER=dev-2\nUSER=broccoli\nGIT_WORKING=/home/$USER/gitworking/mykomap-monolith\nDEPLOY_ENV=/home/$USER/gitworking/deploy.env\nDATA_DIR=/home/$USER/deploy/data\n</code></pre>"},{"location":"deployment/#how-to-install-for-the-first-time","title":"How to install for the first time","text":""},{"location":"deployment/#step-1-set-environment-variables-as-the-application-user","title":"Step 1: set environment variables (as the application user)","text":"<p>The <code>$DEPLOY_ENV</code> file defines some actual environment variables which the deployment process will use. What those are set to will depend on your situation, so this is just a guide.</p> <p>Here is an example of how the <code>$DEPLOY_ENV</code> file may be created as the application user, but with secret values redacted:</p> <pre><code>cat &gt; $DEPLOY_ENV &lt;&lt;EOF\nexport USERDIR=/home/$USER\nexport GIT_WORKING=/home/$USER/gitworking/mykomap-monolith\nexport DEPLOY_DEST=/home/$USER/deploy\nexport DATA_DIR=/home/$USER/deploy/data\nexport WWW_ROOT=/var/www/vhosts/maps.coop/www\nexport APP_ROOT=/var/www/vhosts/maps.coop/www/cwm\nexport VHOST_CONF=/var/www/vhosts/maps.coop/custom.conf\nexport PROXY_PORT=1$UID\nexport PROXY_PATH=/api\nexport DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/$UID/bus\nexport FE_GLITCHTIP_KEY=*REDACTED*\nexport BE_GLITCHTIP_KEY=*REDACTED*\nexport MAPTILER_API_KEY=*REDACTED*\nEOF\n</code></pre> <p>This will hardcode the variables in the file according to the app <code>$USER</code> and <code>$UID</code>, which should already be set by your shell by default. The hardcoding is necessary for Step 3, when we load these environment variables as a different user.</p> <p>Note: paths here should be absolute - relative paths will not work in general.</p> <p>Note: <code>DBUS_SESSION_BUS_ADDRESS</code> should be set in principle be if you are logged in as that user - but in practise is not. This is needed for the deploy script to run <code>systemctl</code> in user-mode.</p> <p>Note: These variables don't strictly have to be defined in a file, it's just convenient for this illustration. You could supply them via other mechanisms.</p>"},{"location":"deployment/#step-2-setup-apache-config-with-elevated-privileges","title":"Step 2: setup Apache config (with elevated privileges)","text":"<p>This step requires elevated privileges, but as it is specific to this application it is not performed via Ansible.</p> <p>It assumes that:</p> <ul> <li>The root directory of the virtual host is at <code>$WWW_ROOT</code></li> <li>A symlink can be created at <code>$APP_ROOT</code> to the directory that   Apache should serve the application content from.</li> <li>A file <code>$VHOST_CONF</code> can be created which contains the Apache   configuration in the context of the application's Virtual host.</li> <li>The user <code>$USER</code> exists already, and its home directory is   accessible to the Apache user. (Typically this means that <code>~$USER</code>   home directory has the \"execute\" flag set allowing the <code>www-data</code>   user group or global access; perhaps by <code>chmod a+x ~$USER</code>.)</li> <li>The user has had linger mode enabled (<code>loginctl enable-linger $USER</code>) to ensure that its DBUS session starts on boot, for use by   user-mode systemd services.</li> </ul> <p>The steps:</p> <pre><code># source this file to get the shared configuration\n. $DEPLOY_ENV\n\n# Link the content to serve into place\nln -sfn $GIT_WORKING/apps/front-end/dist/ $APP_ROOT\n\n# Configure reverse proxying, and symlink following.\ncat &gt; $VHOST_CONF &lt;&lt;EOF\n&lt;Directory $WWW_ROOT/..&gt;\n  Options FollowSymLinks Indexes\n&lt;/Directory&gt;\nProxyPass /api http://localhost:$PROXY_PORT\nProxyPassReverse /api http://localhost:$PROXY_PORT\nEOF\n\nsystemctl reload apache2\n</code></pre>"},{"location":"deployment/#step-3-install-the-app-as-the-application-user","title":"Step 3: install the app (as the application user)","text":"<p>It assumes that:</p> <ul> <li>The public SSH key of <code>$USER</code> has been added as a deploy key for   the <code>cwm-test-data</code>   private repository.</li> <li>This key is used when connecting to github.com. We recommend configuring this in the   user's <code>~/.ssh/config</code> file, since you will may need fetch updates to the data on a regular   basis.</li> </ul> <p>The steps:</p> <pre><code># source this file to get the shared configuration\n. $DEPLOY_ENV\n\n# Create and populate the data directory (an example - the details may vary)\n# Amounts to a git clone --depth=1 git@github.com:DigitalCommons/cwm-test-data\n# Except it works when $GIT_WORKING exists already.\nmkdir -p $DATA_DIR\ncd $DATA_DIR\ngit init\ngit remote add origin git@github.com:DigitalCommons/cwm-test-data.git\ngit fetch --depth=1 # --depth optional\ngit checkout main\n\n# Create and deploy the application source code\nmkdir -p $GIT_WORKING\n\ncd $GIT_WORKING\ngit init\ngit remote add origin git@github.com:DigitalCommons/mykomap-monolith\ngit fetch --depth=1 # --depth optional\ngit checkout main\n\n\n./deploy.sh\n</code></pre> <p>The <code>deploy.sh</code> script will do the rest, including writing the <code>.env</code> files in the applications, which should never be stored in source control.</p>"},{"location":"deployment/#how-to-make-subsequent-updates","title":"How to make subsequent updates","text":"<p>After the initial install, deploying updates is simpler.</p> <p>Note: we assume <code>$DEPLOY_ENV</code> defines our environment, as before.</p> <p>Log in as the application user:</p> <pre><code>ssh $SERVER      # log-in as root\nsu - $USER       # change to the target user\n</code></pre> <p>Then perform the update:</p> <pre><code>cd $GIT_WORKING  # change to the deployed directory\n. $DEPLOY_ENV    # set the configuration\ncd mykomap-monolith\ngit pull         # typically you will need to update the source files in some way\n./deploy.sh      # run the deploy script\n</code></pre> <p>The <code>deploy.sh</code> script will do the rest, including writing the <code>.env</code> files in the applications, which should never be stored in source control.</p>"},{"location":"monorepo/","title":"Monorepo structure","text":"<pre><code>monorepo\n|- apps\n  |- @mykomap/front-end\n  |- @mykomap/back-end\n      |- Dataset class instance for each dataset that needs a deployed backend\n      |- single ts-rest/Fastify server shared across all datasets\n      |- script to generate data for each dataset, consumed by the back-end server\n|- libs\n  |- @mykomap/common\n      |- API ts-rest contract + OpenAPI spec\n      |- prop defs code (used by both front-end and back-end)\n  |- @mykomap/node-utils\n      |- common code that relies on the NodeJS runtime e.g. file utilities\n</code></pre>"},{"location":"monorepo/#dependencies","title":"Dependencies","text":"<p>The build order, set in the <code>build</code> runscript of the root <code>package.json</code> is:</p> <ol> <li><code>libs/common</code></li> <li><code>libs/node-utils</code></li> <li><code>apps/back-end</code></li> <li><code>apps/front-end</code></li> </ol> <p>This is because there is some common code used to label the builds, defined in a file <code>build-info.ts</code>. This ensures the labelling is generated in a consistent way in all the modules.</p> <p>Where does this code go? This gets complicated, as some modules are by design free of NodeJS dependencies, and since those are transitory, <code>libs/common</code> cannot have runtime NodeJS dependent dependencies. (Although, it does have <code>devDependencies</code> on <code>node-utils</code>, because the tests it uses <code>file-utils.ts</code> defined there).</p> <p>However: the <code>build-info.ts</code> code necessarily needs to run <code>git-depend</code> to obtain information about the build, if only at build time.</p> <p>Which means:</p> <ul> <li>The <code>build-info.ts</code> code common to all modules needs to be in the   first module to compile.</li> <li>This module cannot be in <code>node-utils</code>, or else <code>common</code> and   <code>front-end</code> cannot use it.</li> <li>As <code>node-utils</code> needs the <code>build-info.ts</code> code too, at runtime and   build time, therefore it must depend on <code>common</code> rather than vice   versa.</li> </ul> <p>Other modules can then depend on <code>common</code> to get the <code>build-info.ts</code> code, and only need to pull in the <code>spawnSync</code> function from NodeJS in their <code>vite.config.ts</code> files, so the node dependency is only implicit and at build time.</p>"},{"location":"monorepo/#ideas","title":"Ideas","text":"<ul> <li>Use monorepo manager (e.g. Nx, Turborepo) to manage builds</li> <li>Use git subtree, so we can publish sub-folders of the monorepo as independent repos that others   can use. We can set this up later if required, but for now try to ensure repositories are not   tightly coupled unecessarily.</li> <li>Differentiate between maps and datasets: currently a map's data and config is associated with a   single dataset on the backend. We may want to unlink these so that 2 maps can share the same   dataset with differnet config, or a map can show multiple datasets. This could be done in the   future.</li> <li>Allow back-end code to be imported into the front-end for map builds that don't need a separate   back-end.</li> </ul>"},{"location":"server-admin-notes/","title":"In case of server problems","text":""},{"location":"server-admin-notes/#hetzner-cloud","title":"Hetzner Cloud","text":"<p>First port of call is probably Hetzner. The login should be in our password store service. Then you need to navigate to Hetzner's Cloud services (Their Robot site is for bare metal.)</p> <p>Check the graphs to see if the server is under heavy load CPU/IO/Network, has stopped, or is otherwise having problems. There is a dropdown which can switch the view from \"Live\" to an overview of the last hour, day, month, etc.</p> <ul> <li>List of DCC servers: https://console.hetzner.cloud/projects/1450952/servers</li> <li>Prod-2 graphs: https://console.hetzner.cloud/projects/1450952/servers/39640291/graphs</li> <li>Dev-2 graphs: https://console.hetzner.cloud/projects/1450952/servers/36089475/graphs</li> </ul> <p>If it's a load issue, there's a \"Rescale\" tab with which we can upgrade the server to more CPU, Memory and disk space. This should be quick, but it will require a reboot.</p> <p>If it's something which has run amok on the server, it may be resolved by a reboot. Use the reset button on the \"Power\" tab. However, not if the disks are full, and this can't be determined from the Hetzner console directly.</p> <p>Some clues might be available from the server console, which you can open using the link in the \"Actions\" dropdown for the server on the graph page above. This is like taking a peep at the physical monitor of the server, if it had one. If things are looking normal you'll just see a log-in prompt. But you might find some system log messages there which give you a clue.</p>"},{"location":"server-admin-notes/#root-console-things","title":"Root console things","text":"<p>At the time of writing, there is no password access to the root (or in fact any) user accounts on the server, so you can't actually log in via the server console. But you can reset the root password on the \"Rescue\" tab of the Hetzner console, although this will probably trigger a reboot. However, then you can use that to log into the server console.</p> <p>Alternatively someone with access can add your SSH public key, if you have one, to <code>/root/.ssh/authorized_keys</code>, and then you should be able to ssh in via <code>root@prod-2.digitalcommons.coop</code> (insert correct hostname as appropriate)</p> <p>However, when the server is in real trouble, perhaps because it can't cope with the load, typically logging into the console is fraught with problems. In which case using the Hetzner console is better.</p>"},{"location":"server-admin-notes/#whats-the-cpu-memory-load","title":"What's the CPU / Memory load?","text":"<p>On the console this can be done by running the command <code>top</code>, which will show an updating table of processes like this:</p> <pre><code>Tasks: 145 total,   1 running, 144 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  2.9 us,  5.9 sy,  0.0 ni, 91.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nMiB Mem :   7747.9 total,   5298.0 free,    954.0 used,   1496.0 buff/cache\nMiB Swap:      0.0 total,      0.0 free,      0.0 used.   6476.3 avail Mem\n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n   4431 root      20   0   16916  10220   8372 S  11.8   0.1   0:00.04 sshd\n    395 root      19  -1  340468 115548 114352 S   5.9   1.5   0:11.21 systemd-journal\n      1 root      20   0  100908  11816   8416 S   0.0   0.1   0:03.54 systemd\n      2 root      20   0       0      0      0 S   0.0   0.0   0:00.00 kthreadd\n      3 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 rcu_gp\n      4 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 rcu_par_gp\n      5 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 slub_flushwq\n      6 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 netns\n      8 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker/0:0H-events_highpri\n     10 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 mm_percpu_wq\n     11 root      20   0       0      0      0 S   0.0   0.0   0:00.00 rcu_tasks_rude_\n     12 root      20   0       0      0      0 S   0.0   0.0   0:00.00 rcu_tasks_trace\n     13 root      20   0       0      0      0 S   0.0   0.0   0:00.11 ksoftirqd/0\n     14 root      20   0       0      0      0 I   0.0   0.0   0:00.37 rcu_sched\n     15 root      rt   0       0      0      0 S   0.0   0.0   0:00.02 migration/0\n</code></pre> <p>The processes are sorted by load. You can switch it to sort by memory use by pressing the <code>m</code> key. The <code>?</code> key will show brief overview of the keys you can press.</p> <p>To exit, use <code>q</code>.</p>"},{"location":"server-admin-notes/#disk-full","title":"Disk full?","text":"<p>You can check the disk's capacity and free space with <code>df -h</code>. At the time of writing, the output looks like this:</p> <pre><code>root@dev-2:~# df -h\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           775M  936K  774M   1% /run\n/dev/sda1        75G   53G   20G  74% /\ntmpfs           3.8G     0  3.8G   0% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/sda15      253M  6.1M  246M   3% /boot/efi\ntmpfs           775M     0  775M   0% /run/user/0\ntmpfs           775M     0  775M   0% /run/user/7001\ntmpfs           775M     0  775M   0% /run/user/7003\ntmpfs           775M     0  775M   0% /run/user/7002\n</code></pre> <p>This tells us that the main disk <code>/</code> is 75% full, with 20G available. The other partitions can mostly be ignored.</p> <p>If it's near 100% then the server is in trouble. Fixing that requires either increasing the disk size or finding some garbage or otherwise deletable files to remove. If your technical skill is low, using the \"Rescale\" option mentioned in the section above is probably the safest option. However, typical places to look are in <code>/var/</code> - safe things to delete will be in directories named <code>tmp</code> <code>temp</code> <code>cache</code> or similar. Avoid deleting logs unless you really have to, but that's another thing which can fill up. E.g.</p> <pre><code>root@dev-2:~# du -sh /var/*\n2.6M    /var/backups\n121M    /var/cache\n4.0K    /var/crash\n40G     /var/lib\n4.0K    /var/local\n0       /var/lock\n4.5G    /var/log\n4.0K    /var/mail\n4.0K    /var/opt\n0       /var/run\n28K     /var/spool\n14M     /var/tmp\n112M    /var/www\nroot@dev-2:~# du -sh /var/tmp.*\ndu: cannot access '/var/tmp.*': No such file or directory\nroot@dev-2:~# du -sh /var/tmp/*\n4.0K    /var/tmp/cloud-init\n8.0K    /var/tmp/systemd-private-5d313365530846358c648b6ed26b16fd-apache2.service-k3rsZh\n8.0K    /var/tmp/systemd-private-5d313365530846358c648b6ed26b16fd-systemd-logind.service-nMa2no\n8.0K    /var/tmp/systemd-private-5d313365530846358c648b6ed26b16fd-systemd-resolved.service-bGGS1b\n8.0K    /var/tmp/systemd-private-5d313365530846358c648b6ed26b16fd-systemd-timesyncd.service-jAqmK6\n14M     /var/tmp/virtuoso\nroot@dev-2:~# du -sh /var/log/*\n4.0K    /var/log/alternatives.log\n12K     /var/log/alternatives.log.1\n4.0K    /var/log/alternatives.log.10.gz\n4.0K    /var/log/alternatives.log.11.gz\n4.0K    /var/log/alternatives.log.12.gz\n4.0K    /var/log/alternatives.log.2.gz\n4.0K    /var/log/alternatives.log.3.gz\n[...elided]\n</code></pre> <p>If you're really hunting for the culprit, you can get a sorted list of directories and files by size using:</p> <pre><code>root@dev-2:~# du -cax / | sort -rn | tee filesizes.txt | head\n55008892        total\n55008892        /\n46799932        /var\n41895664        /var/lib\n33852844        /var/lib/mysql\n32256420        /var/lib/mysql/property_boundaries\n18362384        /var/lib/mysql/property_boundaries/land_ownership_polygons.ibd\n11341836        /var/lib/mysql/property_boundaries/pending_inspire_polygons.ibd\n7584452         /var/lib/backups\n7584448         /var/lib/backups/property_boundaries.borg\n</code></pre> <p>This also writes the list to <code>filesizes.txt</code> which you can inspect with <code>less filesizes.txt</code> (press the 'h' key after running it for help, or run <code>man less</code> for the manual page)</p>"},{"location":"server-admin-notes/#rebooting-from-the-root-console","title":"Rebooting from the root console","text":"<p>You can do that like this:</p> <pre><code>root@dev-2:~# reboot\n</code></pre> <p>There's also a <code>halt</code> command, but the server will stay off if you use that.</p>"},{"location":"server-admin-notes/#monitoring-and-restarting-the-mykomap-server","title":"Monitoring and restarting the mykomap server","text":""},{"location":"server-admin-notes/#as-the-application-user","title":"As the application user...","text":"<p>Usually devs do this when logged in as the user running the application - in this case, on dev-2 and prod-2, this will be <code>broccoli</code>.</p> <p>This shows (a simple case of) how the server is rebuilt. For more and more accurate details, see the mykomap-monolith deployment documentation.</p> <pre><code>root@dev-2:~# su - broccoli               # switch from root to the broccoli user\n\nbroccoli@dev-2:~$ cd ~/deploy/data/       # switch to the data directory\n\nbroccoli@dev-2:~/deploy/data$ git pull    # pull the latest data (simple case, assumes no branch switching needed)\nAlready up to date.\n\nbroccoli@dev-2:~/deploy/data$ cd ~/gitworking/mykomap-monolith/            # switch to the app directory\n\nbroccoli@dev-2:~/gitworking/mykomap-monolith$ git pull\nAlready up to date.\n\nbroccoli@dev-2:~/gitworking/mykomap-monolith$ . ~/gitworking/deploy.env   # load the environment variables needed to rebuild the app and restart the service\n\nbroccoli@dev-2:~/gitworking/mykomap-monolith$ systemctl status --user mykomap-backend.service  # Check the app service status\n\u25cf mykomap-backend.service - Mykomap back-end process manager for broccoli\n     Loaded: loaded (/home/broccoli/.config/systemd/user/mykomap-backend.service; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2024-11-24 17:48:10 UTC; 6min ago\n       Docs: https://github.com/DigitalCommons/mykomap-monolith/\n   Main PID: 4056 (npm run start:a)\n      Tasks: 35 (limit: 9242)\n     Memory: 151.5M\n        CPU: 12.869s\n     CGroup: /user.slice/user-7002.slice/user@7002.service/app.slice/mykomap-backend.service\n             \u251c\u25004056 \"npm run start:attached\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n             \u251c\u25004203 sh -c \"npm run start\"\n             \u251c\u25004204 \"npm run start\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n             \u251c\u25004242 sh -c \"node ./start.js\"\n             \u2514\u25004243 node ./start.js\n\nNov 24 17:54:34 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470874892,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1d\",\"req\":{\"method\":\"GET\",\"url\":\"/dataset/delhi/search?filter%5B0%5D=data_sources%3A&gt;\nNov 24 17:54:34 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470874910,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1d\",\"res\":{\"statusCode\":200},\"responseTime\":17.900402000173926,\"msg\":\"request comple&gt;\nNov 24 17:54:40 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470880871,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1e\",\"req\":{\"method\":\"GET\",\"url\":\"/dataset/delhi/search?filter%5B0%5D=data_sources%3A&gt;\nNov 24 17:54:40 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470880902,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1e\",\"res\":{\"statusCode\":200},\"responseTime\":30.778594000265002,\"msg\":\"request comple&gt;\nNov 24 17:54:41 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470881048,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1f\",\"req\":{\"method\":\"GET\",\"url\":\"/dataset/delhi/search?filter%5B0%5D=data_sources%3A&gt;\nNov 24 17:54:41 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470881067,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1f\",\"res\":{\"statusCode\":200},\"responseTime\":19.006068999879062,\"msg\":\"request comple&gt;\nNov 24 17:54:41 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470881068,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1g\",\"req\":{\"method\":\"GET\",\"url\":\"/dataset/delhi/search?filter%5B0%5D=data_sources%3A&gt;\nNov 24 17:54:41 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470881107,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1g\",\"res\":{\"statusCode\":200},\"responseTime\":38.5415049996227,\"msg\":\"request complete&gt;\nNov 24 17:54:41 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470881209,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1h\",\"req\":{\"method\":\"GET\",\"url\":\"/dataset/delhi/search?filter%5B0%5D=data_sources%3A&gt;\nNov 24 17:54:41 dev-2.digitalcommons.coop bash[4243]: {\"level\":30,\"time\":1732470881238,\"pid\":4243,\"hostname\":\"dev-2.digitalcommons.coop\",\"reqId\":\"req-1h\",\"res\":{\"statusCode\":200},\"responseTime\":28.585051000118256,\"msg\":\"request comple&gt;\n\nbroccoli@dev-2:~/gitworking/mykomap-monolith$ ./deploy.sh # start the rebuild. should restart the server automatically\n\n[... detailed output elided]\n\n</code></pre>"},{"location":"server-admin-notes/#as-the-root-user","title":"As the root user...","text":"<p>But this can also be done as the root user, perhaps with more ease, because of one less step. In this example, the server status is checked, then it is restarted, then started (which should do nothing if it already started) then the status is checked again. You can see the process number and memory usage change.</p> <pre><code>root@dev-2:~# systemctl --machine broccoli@.host --user  status mykomap-backend\n\u25cf mykomap-backend.service - Mykomap back-end process manager for broccoli\n     Loaded: loaded (/home/broccoli/.config/systemd/user/mykomap-backend.service; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2024-11-24 17:14:53 UTC; 33min ago\n       Docs: https://github.com/DigitalCommons/mykomap-monolith/\n   Main PID: 3498\n      Tasks: 35 (limit: 9242)\n     Memory: 168.9M\n        CPU: 8.240s\n     CGroup: /user.slice/user-7002.slice/user@7002.service/app.slice/mykomap-backend.service\n             \u251c\u25003498 \"npm run start:attached\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n             \u251c\u25003643 sh -c \"npm run start\"\n             \u251c\u25003644 \"npm run start\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n             \u251c\u25003682 sh -c \"node ./start.js\"\n             \u2514\u25003683 node ./start.js\n\nroot@dev-2:~# systemctl --machine broccoli@.host --user  restart mykomap-backend\n\nroot@dev-2:~# systemctl --machine broccoli@.host --user  start mykomap-backend\n\nroot@dev-2:~# systemctl --machine broccoli@.host --user  status mykomap-backend\n\u25cf mykomap-backend.service - Mykomap back-end process manager for broccoli\n     Loaded: loaded (/home/broccoli/.config/systemd/user/mykomap-backend.service; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2024-11-24 17:48:10 UTC; 8s ago\n       Docs: https://github.com/DigitalCommons/mykomap-monolith/\n   Main PID: 4056\n      Tasks: 35 (limit: 9242)\n     Memory: 196.5M\n        CPU: 2.977s\n     CGroup: /user.slice/user-7002.slice/user@7002.service/app.slice/mykomap-backend.service\n             \u251c\u25004056 \"npm run start:attached\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n             \u251c\u25004203 sh -c \"npm run start\"\n             \u251c\u25004204 \"npm run start\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n             \u251c\u25004242 sh -c \"node ./start.js\"\n             \u2514\u25004243 node ./start.js\n</code></pre>"},{"location":"versioning-and-releases/","title":"Versioning and Releases","text":"<p>Here we describe a suggested process for releasing code. Specifically with regard to consistent labelling which is required for matching issues tracked using the Sentry API to the code context in which they occurred.</p> <p>[!NOTE]</p> <p>This document is a work in progress.</p> <p>It was written as a starting point for a more comprehensive guide, following a suggestion in a PR.</p>"},{"location":"versioning-and-releases/#background","title":"Background","text":"<p>The Sentry API is a mechanism for reporting errors in deployed applications. We use an open source service supporting the API, called Glitchtip, to receive them.</p> <p>When an error is detected, a call is made to the API with some tags and other metrics characterising the problem. One of these tags is the Sentry API release tag. This is matched to source maps uploaded previously to the API, allowing minified code, which is fairly unintelligible, to be shown in un-minified form. The match is done based on the filename and the release tag (and the dist tag if one is set).</p> <p>In addition, the release tag and some other information is shown in the Glitchtip issue report. These need to be sufficient to allow developers inspecting these report to identify the version of the code-base in which the issue was created, so needs the commit ID in addition to the release.</p> <p>So in order to get legible source code when viewing the report:</p> <ul> <li>We need consistent release tags created in the build.</li> <li>The source map files need to be deployed with the (minified)   source. (In the front-end at least.)</li> <li>These need to be labelled with the appropriate release tag.</li> <li>The release tag also needs to be included in the application's   initial Sentry API initialisation.</li> </ul> <p>To facilitate that:</p> <ul> <li>All Mykomap modules use the same mechanism to deduce the build info   at build time and insert it into the source code.</li> <li>This is reported by the back-end module's <code>version</code> endpoint, the on   the console in the front-end.</li> <li>The version is inferred from the last git tag with the format   <code>v&lt;semantic-version&gt;</code> where the semantic version is a sequence of   positive integers delimited with periods. e.g. <code>v4.1.3</code></li> <li>The \"build description\" is generated using <code>git describe</code> and has   the form <code>v&lt;semantic-version&gt;-&lt;commits&gt;-g&lt;commit-id&gt;[-dirty]</code>,   e.g. <code>v4.1.3-23-b3dfba0</code>, where:</li> <li><code>v&lt;semantic-version&gt;</code> is the last matching version tag</li> <li><code>&lt;commits&gt;</code> is the number of commits on this branch since that tagged commit</li> <li><code>&lt;commit-id&gt;</code> is the current truncated commit ID, containing 7 or     more characters.</li> <li><code>-dirty</code> is appended if the repository working directory has any     modifications on build.</li> <li>The semantic version is inserted into each modules' <code>package.json</code>   as the <code>\"version\"</code> attribute.</li> <li>The Sentry API release tag is derived from it, and has the form   <code>&lt;module-name&gt;@&lt;semantic-version&gt;</code>, where the former is the module   name, modified to follow the rules defined here and   the latter is the semantic version above.   e.g. <code>@mykomapfront-end@4.0.0</code> (the slash after <code>@mykomap/</code> is   illegal and is removed.) The former is included to distinguish the   modules.</li> <li>It is inserted into the front end module's <code>package.json</code> in the   attribute <code>\"config.sentry.release\"</code>.</li> </ul>"},{"location":"versioning-and-releases/#process","title":"Process","text":"<p>This outlines a process for versioning and releases. The concepts are still important - however, an <code>npm</code> run-script has been added to help automate this, see the section below, Automation of release tagging.</p>"},{"location":"versioning-and-releases/#tagging-releases","title":"Tagging releases","text":"<p>The remainder of this process assumes that commits the repository will have periodic semantic versioning tags applied, of the form <code>v&lt;dotted integers&gt;</code>. \"Semantic versioning\" boils down to:</p> <p>Given a version number <code>MAJOR.MINOR.PATCH</code>, increment the:</p> <ol> <li>MAJOR version when you make incompatible API changes</li> <li>MINOR version when you add functionality in a backward compatible manner</li> <li>PATCH version when you make backward compatible bug fixes</li> </ol> <p>Additional labels for pre-release and build metadata are available as extensions to the <code>MAJOR.MINOR.PATCH</code> format. _[Presumably as</p> <p>supplemental dotted integers, in our scheme]_</p> <p>The current version of Mykomap has a major version of 4, following on from previous versions, of which it is an entire rewrite, as well as incompatible to (insofar as it even had an API).</p> <p>Our version of Mykomap does have an API, exposed by the back-end, and consumed by the front end. As these are intended to interoperate, we store both, plus libraries with shared data, in a \"monorepo\" - a single repository containing all of the related projects. Although we would like to maintain some independence between the components, and possibly allow them to be split apart again in the future, this does mean there is in practice a form of strong coupling between them - if not explicit, then possibly inadvertent couplings are likely to exist.</p> <p>And as such all components naturally have the same semantic version, stamped in their <code>package.json</code> files' <code>\"version\"</code> attribute.</p> <p>However, the API is intended to be consumed - or to be able to be - by external systems. Which is clearly the main sense a semantic versioning system would apply here. Therefore we should be tagging our releases to reflect changes in that.</p> <p>A second consideration is the idea that we are (at least in practice) aiming to use a modified version of \"trunk based development\". I say \"modified\" as we now seem to have a distinct <code>dev</code> branch as well as a <code>main</code> branch, but the former should always be a fast-forward-merge away from the latter, so can be seen as a continuous branch.</p> <p>Tags should be placed on this branch. The semantics of <code>main</code> are such that anything on that branch is (or could be) deployed in production, and so should be given a release tag.</p> <p>So to sum up, I think we should:</p> <ul> <li>Create a new version tag every time something is deployed to   production</li> <li>Use the semantic versioning rules above to guide the selection of   these versions</li> </ul>"},{"location":"versioning-and-releases/#automation-of-release-tagging-npm-run-release","title":"Automation of release tagging: <code>npm run release</code>","text":"<p>The TL:DR; here is that to create a release with a tagged commit, you can now run the following from the mykomap-monolith project root directory:</p> <pre><code># To tag a new release with a sem-ver v4.1.3:\nnpm run release v4.1.3\n</code></pre> <p>...Which will tag the repository and rebuild it, updating the <code>package.json</code> and <code>package-lock.json</code> files for you, then squash the result into one commit tagged <code>v4.1.3</code>. After which, you can manually push the result:</p> <pre><code>git push\ngit push --tags\n</code></pre> <p>...Or, inspect and delete the temporary branch created:</p> <pre><code>git branch -D prepare-release-v4.1.3\n</code></pre> <p>This automates the manual process, which is roughly as follows. Although note that the script intentionally follows a slightly different process involving a squash merge, to allow for inspection of failures.</p> <p>The manual process is:</p> <ul> <li>Ensure the code builds cleanly:   <code>npm run clean &amp;&amp; npm ci &amp;&amp; npm run build &amp;&amp; npm run test</code></li> <li>Ensure any changes are committed, if this succeeds:   <code>git add -u &amp;&amp; git commit -m \"...your commit message here...\"</code></li> <li>Tag the commit with the version:   <code>git tag v4.1.3</code></li> <li>Rebuild to apply the tags to the <code>package.json</code> files:   <code>npm run build</code></li> <li>Updateg the <code>package-lock.json</code> with the tags too:   <code>npm install --package-lock-only</code></li> <li>Commit that to the same commit:   <code>git add -u &amp;&amp; git commit --amend -c HEAD</code></li> </ul> <p>The reason these all need to be squashed into the one commit with a tag applied is so that GitHub (and possibly other usages) will build and archive the actual deployable code for that version. If in practise a deployable release needs extra changes following the tagged release commit (as they do in practise) they'll be omitted from the release archive created by GitHub if they aren't squashed into it.</p>"},{"location":"versioning-and-releases/#reflections","title":"Reflections","text":"<p>A consequence is that deployments using <code>dev</code> will have the version of the last production build, just with a different build specification.</p> <p>Possibly this is unhelpful - in retrospect I wonder if we should adjust our build labelling to special-case this region beyond <code>main</code> and give them release tags which clearly identify them as different, and \"in development\".</p> <p>An alternative might be to extend the semantic versioning beyond <code>main</code> and onto <code>dev</code>, with the idea that this is a continuous process. That would however beg the question about why we need <code>dev</code>, perhaps we only need <code>main</code>.</p> <p>A third might be to adopt the even/odd versioning schemes used by some software: minor version numbers which are even are considered \"development releases\", and those which are odd are \"production releases\". Then the <code>dev</code> branch would be tagged with even minor versions, and a transition to releasing for production would require switching to odd minor versions. But I think this would imply we should change our deployment and sprints processes accordingly, to acknowledge this distinction and partitioned our work into attending to development of new features, release of those features to production, and post-release bug-fixes.</p> <p>Disclaimer: all of these reflections are beyond the scope of current work, and need to be discussed by the team.</p>"},{"location":"versioning-and-releases/#preparing-the-code-for-a-deploy","title":"Preparing the code for a deploy","text":"<p>When preparing to deploy, locally in the development working directory, we need to:</p> <ul> <li>Ensure the tests all run successfully. <code>npm run tests</code></li> <li>Apply any Git version tag you want to appear in the build, first (in   the format described above). e.g. <code>git tag v4.1.3</code>.</li> <li>Run the build - the version attributes will be updated in all the   <code>package.json</code> files, and the Sentry release tag in the front-end's.</li> <li>Commit these changes to Git.</li> <li><code>git push</code> the changes.</li> </ul>"},{"location":"versioning-and-releases/#deploying","title":"Deploying","text":"<p>On the server where the code is being deployed:</p> <ul> <li>Ensure <code>apps/front-end/.env</code> includes the environment parameters (as described below).</li> <li>Check out the correct commit.</li> <li>Ensure there are no other uncommitted changes (to avoid the <code>-dirty</code>   suffix).</li> <li>Run the build - the code will be tagged with the correct build info,   including the commit ID.</li> <li>Invoke the front-end module's run-script <code>upload-sourcemaps</code> to   upload the source-map files via the Sentry API.</li> </ul> <p>The environment parameters needed by <code>npm run upload-sourcemaps</code> are:</p> <ul> <li><code>GLITCHTIP_KEY</code> - obtain this from our Glitchtip account, it's the   alphanumeric code set in the <code>glitchtip_key</code> parameter of the   Security Endpoint URL, found under Settings -&gt; Projects -&gt;   @mykomap/front-end.</li> <li><code>SENTRY_AUTH_TOKEN</code> - obtain this from our Glitchtip account, there   is one generated already, called SentryCli, accessible under   Profile -&gt; Auth Tokens.</li> <li><code>SENTRY_URL</code> - typically <code>https://app.glitchtip.com</code></li> <li><code>SENTRY_ORG</code> - should be <code>digital-commons-coop</code></li> <li><code>SENTRY_PROJECT</code> - should be <code>mykomapfront-end</code></li> </ul> <p>[!NOTE]</p> <p>The environment parameters set in the <code>deploy.sh</code> script are slightly different, because of the wider context there. See the comments in the script itself for details.</p> <p>[!NOTE]</p> <p>The upload is performed in a run-script by the Sentry CLI. It could in future use the Sentry Vite plugin, which may be simpler.</p>"}]}